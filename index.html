<!doctype html>
<html>
  <head>
    <script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>NeuroPhi by Markus Woodson and Danielle Rager</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>
      <div id="header">
        <nav>
          <li class="fork"><a href="https://github.com/mwoodson1/15418FinalProject">View On GitHub</a></li>
          <li class="downloads"><a href="https://github.com/mwoodson1/15418FinalProject/zipball/master">ZIP</a></li>
          <li class="downloads"><a href="https://github.com/mwoodson1/15418FinalProject/tarball/master">TAR</a></li>
          <li class="title">DOWNLOADS</li>
        </nav>
      </div><!-- end header -->

    <div class="wrapper">

      <section>
        <div id="title">
          <h1>NeuroPhi</h1>
          <p></p>
          <hr>
          <span class="credits left">Project maintained by <a href="https://github.com/mwoodson1">mwoodson1</a>,<a href="https://github.com/dmrager">dmrager</a></span>
          <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
        </div>

        <h3>

<a href="proposal.html">Proposal</a>
<a href="checkpoint.html">Checkpoint</a>

<h3>Summary</h3>
<p>Neurophi is a C++ framework with a MATLAB interface for doing fast, parallel convolutional neural network (CNN) training, particularly for large, high-res images.</p>
<p></p>

<h3>Motivation</h3>
<p>There is increasing interest in classifying large, high-resolution images. Social networking sites like Facebook must do fast, automated facial recognition of user-uploaded photos. As a result, the <a href="http://www.image-net.org/">ImageNet</a> database of large images is replacing many legacy databases of 32x32 images to become the new standard for training neural networks to do image classification. In some applications such as image denoising, it also beneficial to convolve an image with a large filter. However, many existing parallel CNN training frameworks, including Caffe and CudaConvnet, continue to optimize their convolutions for small images and kernels. NeuroPhi differentiates itself from existing parallel CNN training frameworks by computing convolutions in the Fourier domain instead of the time domain, which is optimal for large images and kernels. 
Unlike the existing parallel CNN training frameworks that rely on CUDA and GPU computation, NeuroPhi's target hardware is a CPU and Intel Xeon Phi Coprocessor. Neural network operations are computationally dense and highly parallelizable; as GPU core counts continue to increase, GPU ANN training is not compute limited, but bandwith limited by the rate at which data can be offloaded to the GPU. The Xeon Phi coprocessor, Intel's competitor to NVIDIA's Tesla, supports code written for x86  architecture, and therefore has the potential to run ANN training algorithms with fewer bandwidth limited offloads. By writing NeuroPhi for CPU and Xeon Phis, we also give users the convenience of extending the framework with standard C++ parallelization tools including OpenMP, Cilk, and ISPC.</p>


<h3>Background</h3>
<p>Artificial neural networks (ANNs) are a biologically-inspired machine learning method of solving classes of problems that require learning. Given a class of simple functions <i>F</i> and a set of observations <i>O</i>, an ANN will learn the transfer functions for each node or neuron in one or more hidden layers of a network such that the network optimally performs feature extraction and classification. The ability to learn using a hierarchy of simple transfer functions has made ANNs key computational models for applications such as computer vision and speech recognition. However, large neural networks--which may have several hidden layers, thousands to millions of nodes per layer, and thousands to millions of edges between layers--are very computationally expensive to train.

A common supervised ANN training algorithm known as backpropagation iteratively updates the weights of the network to minimize a loss function--the difference between a known, desired network output and the current network output--by determining the contribution of each input weight and bias in the network to the loss function. The basic steps of backpropagation are as follows</p>

<ul>
  <li>Initialize the input layer of the network to include an input for bias</li>
  <li>Propagate the activity forward for each layer of the network</li>
  <li>Calculate the error at the output layer</li>
  <li>Back-propagate the error through the other network layers</li>
  <li>Update the weights of the network</li>
</ul>

<p>
Convolutional neural networks are a subclass of feed-forward ANNs that reduce the number of parameters required to do efficient feature extraction by applying the same filter(s) to several contiguous regions of an image. The feature extraction phase of the learning process may have many layers, including, most notably:
</p>

<b>Convolutional layers</b>
<p>
  The input parameters of convolutional layers are a set of learnable filters. Each one of these filters is then convolved with the input volume to produce its output. The process of convolution can be expressed by the following formula in 1D
</br>
  <img src="1dConvolution.jpg">
</br>
  In 2D this can be visualized as the following:
</br>
  <img src="kernel_convolution.jpg">
</br>
  The following is a naive implementation of 2D convolution:
  <pre><code>for w in 1..W
  for h in 1..H
    for x in 1..K
      for y in 1..K
        output(w, h) += input(w+x, h+y) * filter(x, y)
      end
    end
  end
end
</code></pre>
  Most CNN libraries, including <a href="http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1ConvolutionLayer.html">Caffe</a>,  use linear algebra tricks to manipulate the matrices involved in the convolution and take advantage of fast matrix multiplication with cblas. In particular one can "stack" the filters on top of one another and do 1 large convolution instead of several smaller ones.
</p>

</br>
<b>Pooling layers</b>
<p>
  In order to reduce output variance, pooling layers simply take the max or average value of a feature over a region of the image. This ensures that the network will have the same output for small translations of the input image.
</p>

</br>
<p>
The feature extraction layers are followed classification architecture, which generally consists of...
</p>
<b>Fully connected layers</b>
<p>
These layers consist of the classic artificial neural network computational units with all-to-all connections between layers:
</p>
</br>
<img src="fully_connected.png">
<p>
</p>

<h3>Approach</h3>
<p>We began our work from the existing <a href="https://github.com/sdemyanov/ConvNet">ConvNet</a> framework, which supports serial CNN training on a CPU, very minimal parallelization for CNN training on a CPU, and GPU CNN training using <a href="https://code.google.com/p/cuda-convnet2/">CudaConvnet</a>. We aimed to dramatically increase the parallelization of the CPU version of ConvNet, make it suitable for doing CNN training/classification on large images, and offload some of its computations to the Xeon Phi coprocessor.</p>
<p>Neural network training consists of forward propagation, backwards propagation, and weight updates of the various layeys of the network. We timed ConvNet's serial CPU implementation of CNN training in the computationally intensive stages of forward propagation, backwards propagation, and weight updates for the convolutional, fully-connected, and scaling layers, and determined that most of the time during network training was spent on convolutional layer computations. Back propgation through the convolutional layer was particularly slow, and this training stage is dominated by image convolutions. We therefore decided to focus our efforts on optimizing the parallelization of the actual convolution operation.</p>
<p>NeuroPhi's convolutional layer uses Fast Fourier Transforms, which transform the convolution operator into a simple element-wise product of the FFT of the image and FFT of the kernel. Though there is overhead in computing an FFT and inverse FFT, the convolution in the Fourier domain is highly vectorizable and is only O(nlogn) compared to the O(n^2) time domain implementation.</p>
<p>One common complaint of the FFT convolution implementation is that it doubles the memory useage of the convolution operation, since the convolution kernel must be padded to match the size of the input. However, a trick can be employed for convolutions applied to images. A convolution input typically has a real and imaginary component. Since images have only real components, the expanded kernel can be made to occupy the "complex" portion of the image. This halves the memory useage of the computation and approximately halves the latency of the operation, since only one FFT is now required.</p>
<p>Another important optimization for a parallel CNN training framework is concurrent training on multiple images, known as parallel batch processing. New network weights are calculated concurrently over multiple images, and the weights from several images are then reduced so that a single weight change is applied to the network for a batch of images. This can be acheived by multithreading image computations through the network and heeding the critical region in which weight computations across images (threads) are summed.</p>
<p></p>
<h3>Results</h3>
All results below were obtained by running code on a single node of <code>latedays</code>, on two, six-core Xeon e5-2620 v3 processors.

<p>We compared three implementations of image convolution: 1) the naive, time domain implementation pictured in the  background section 2) A recursive FFT implementation of convolution, new to NeuroPhi, and 3) An unrolled FFT implementation of convolution, new to NeuroPhi. Convolution runtimes on 512x512 pixel images (roughly the average size of images in the ImagNet database) for serial versions of the three algorithms as a function of kernel size are shown below. While runtime quickly blows up for large kernel sizes in the time domain, it remains fairly constant over kernel sizes in the two Fourier domain algorithms.</p>   
<p>We then implemented parallelized versions of all three algorithms. The time domain algorithm was parallelized with OpenMP multi-threading and ISPC vector intrinsics. The recursive FFT algorithm was parellized with <code>cilk_spawn</code>'s fork-and-join parallelism. The unrolled FFT algorithm was already so fast after parallelizing it with ISPC vector intrinsics that spawning new threads was generally too much overhead and just incurred latency. Cilk had too much overhead and only a single for loop was efficiently parallelized with OpenMP. It is also worth noting that while OpenMP provides <code>#pragma vector</code> and <code>#pragma simd</code>  directives for vector operations, we got signficantly better speedup by writing vector intrinsics with ISPC. Runtimes of all three parallel implementations on 512x512 pixel images as a function of kernel size are shown below. Once again, while runtime quickly blows up for large kernel sizes in the parallel time domain algorithm, it remains fairly constant over kernel sizes in the two parallel Fourier domain algorithms.</p>
<p>Serial implementation to parallel implementation speed-ups for the three algorithms are shown below. </p>
<p>Our fastest convolution performance was acheived using the unrolled, vectorized FFT with the complex component convolution trick previously described, manual control of the thread count to match the number of execution contexts on a latedays machine, and the THREAD_AFFINITY compiler flag, which...</p>

<img src="LateDaysResults_SmallImages.png">
<p></p>
<p>We observe that the time-domain convolutional layer implementation outperforms both FFT implementations for small kernel sizes, but the unrolled FFT begins to outperform the time domain implementation as kernel size increases. The effeciency of convolutions in the Fourier domain is dependent on image size as well as kernel size. Though many popular neural network image datasets use 32x32, many images that one might want to classify with a CNN in practice are much larger. We tested our algorithms on the average size of images found in <a href="http://www.image-net.org/">ImageNet</a>, a premier dataset of large images for neural network training. The run times for the three convolutional layer implementations for 512x512 images as a function of kernel size are shown below. FFT implementations far outperform the time domain implementation for large images and kernel sizes.</p>
<img src="LateDaysResults_LargeImages.png">

<h3>Challenges and Future Directions</h3>



  </body>
</html>
