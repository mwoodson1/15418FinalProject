<!doctype html>
<html>
  <head>
    <script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>NeuroPhi by Markus Woodson and Danielle Rager</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>
      <div id="header">
        <nav>
          <li class="fork"><a href="https://github.com/mwoodson1/15418FinalProject">View On GitHub</a></li>
          <li class="downloads"><a href="https://github.com/mwoodson1/15418FinalProject/zipball/master">ZIP</a></li>
          <li class="downloads"><a href="https://github.com/mwoodson1/15418FinalProject/tarball/master">TAR</a></li>
          <li class="title">DOWNLOADS</li>
        </nav>
      </div><!-- end header -->

    <div class="wrapper">

      <section>
        <div id="title">
          <h1>NeuroPhi</h1>
          <p></p>
          <hr>
          <span class="credits left">Project maintained by <a href="https://github.com/mwoodson1">mwoodson1</a>,<a href="https://github.com/dmrager">dmrager</a></span>
          <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
        </div>

        <h3>

<a href="proposal.html">Proposal</a>
<a href="checkpoint.html">Checkpoint</a>

<h3>Summary</h3>
<p>Neurophi is a C++ framework with a MATLAB interface for doing fast, parallel convolutional neural network (CNN) training, particularly for large image sizes. Unlike the existing parallel CNN training frameworks that rely on CUDA and GPU computation, NeuroPhi's target hardware is a CPU and Intel Xeon Phi Coprocessor. NeuroPhi therefore supports code written for standard x86 architecture and can be extended using standard C++ parallelization tools including OpenMP, Cilk, and ISPC. NeuroPhi also differentiates itself from existing parallel CNN training frameworks by computing convolutions in the Fourier domain, which is optimal for large images and kernels, and by parallelizing mini-batch training of the network across cores. </p>
<p></p>

<h3>Background</h3>
<p>Artificial neural networks (ANNs) are a biologically-inspired machine learning method of solving classes of problems that require learning. Given a class of simple functions <i>F</i> and a set of observations <i>O</i>, an ANN will learn the transfer functions for each node or neuron in one or more hidden layers of a network such that the network optimally solves a specified problem. The ability to learn using a hierarchy of simple transfer functions has made ANNs key computational models for applications such as computer vision and speech recognition. However, large neural networks--which may have several hidden layers, thousands to millions of nodes per layer, and thousands to millions of edges between layers--are very computationally expensive to train.

A common supervised ANN training algorithm known as backpropagation iteratively updates the weights of the network to minimize a loss function--the difference between a known, desired network output and the current network output--by determining the contribution of each input weight and bias in the network to the loss function. The basic steps of backpropagation are as follows</p>

<ul>
  <li>Initialize the input layer of the network to include an input for bias</li>
  <li>Propagate the activity forward for each layer of the network</li>
  <li>Calculate the error at the output layer</li>
  <li>Back-propagate the error through the other network layers</li>
  <li>Update the weights of the network</li>
</ul>

<p>
A convolutional neural network is similar to a classic artificial neural network, but allows for different types of layers in the network. Some of the key layers are outlines below:
</p>

<b>Convolutional layer</b>
<p>
  This layers input parameters are a set of learnable filters. Each one of these filters is then convolved with the input volume to produce its output. The process of convolution can be expressed by the following formula in 1D
</br>
  <img src="1dConvolution.jpg">
</br>
  In 2D this can be visualized as the following:
</br>
  <img src="kernel_convolution.jpg">
</br>
  The following is a naive implementation of 2D convolution:
  <pre><code>for w in 1..W
  for h in 1..H
    for x in 1..K
      for y in 1..K
        output(w, h) += input(w+x, h+y) * filter(x, y)
      end
    end
  end
end
</code></pre>
  Most CNN libraries use clever tricks to manipulate the matrices involved in the convolution and take advantage of fast matrix multiplication with cblas. In particular one can "stack" the filters on top of one another and do 1 large convolution instead of several smaller ones.  <!-- -----More on this can be read ADD LINK HERE TO CAFFE CONV IMP------- -->
</p>

</br>
<b>Pooling layer</b>
<p>
  In order to reduce output variance, pooling layers simply take the max or average value of a feature over a region of the image. This ensures that the network will have the same output for small translations of the input image.
</p>

</br>
<b>Fully connected</b>
<p>
This is a classic artificial neural network layer, visualized below:
</p>
</br>
<img src="fully_connected.png">
<p>
</p>

<p>
CNN's have had great success in image classification and face recognition tasks but most, if not all, of the implementations used GPU implementations for speed. 
</p>

<h3>Approach</h3>
We began our work from the existing <a href="https://github.com/sdemyanov/ConvNet">ConvNet</a> framework, which supports serial CNN training on a CPU, very minimal parallelization for CNN training on a CPU, and GPU CNN training using <a href="https://code.google.com/p/cuda-convnet2/">CudaConvnet</a>. We aimed to dramatically increase the parallelization of the CPU version of ConvNet, make it suitable for doing CNN training/classification on large images, and offload some of its computations to the Xeon Phi coprocessor.  We determined that most of the time during network training was spent on convolutional layer computations. We therefore decided to focus our efforts on optimizing the parallelization of the actual convolution operation. NeuroPhi's convolutional layer uses Fast Fourier Transforms, which transform the convolution operator into a simple element-wise product of the FFT of the image and FFT of the kernel. We also parallelized the image batch-training process by doing concurrent training on the images within a batch. The results of the training across images are then reduced across cores and the network weights are updated accordingly.
<p></p>
<h3>Results</h3>
The results below were obtained by running code on a single node of <code>latedays</code>, on two, six-core Xeon e5-2620 v3 processors.

<p>We compared three parallelized implementations of image convolution: 1) the naive, time domain implementation shown above, which was based upon ConvNet's current convolution operation, but further parallelized with Cilk and ISPC vector intrisics. 2) A recursive FFT implementation of convolution, new to NeuroPhi, and 3) An unrolled FFT implementation of convolution, new to NeuroPhi. The runtimes of all three implementations on the latedays cluster as a function of kernel size are shown below for 32x32 images.</p>
<img src="LateDaysResults_SmallImages.png">
<p></p>
<p>We observe that the time-domain convolutional layer implementation outperforms both FFT implementations for small kernel sizes, but the unrolled FFT begins to outperform the time domain implementation as kernel size increases. The effeciency of convolutions in the Fourier domain is dependent on image size as well as kernel size. Though many popular neural network image datasets use 32x32, many images that one might want to classify with a CNN in practice are much larger. The run times for the three convolutional layer implementations for 512x512 images as a function of kernel size are shown below. FFT implementations far outperform the time domain implementation for large images and kernel sizes.</p>
<img src="LateDaysResults_LargeImages.png">



  </body>
</html>
