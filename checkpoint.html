<!doctype html>
<html>
  <head>
    <script type="text/javascript" src="http://latex.codecogs.com/latexit.js"></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>NeuroPhi by Markus Woodson and Dannielle Rager</title>
    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.7.1/jquery.min.js"></script>
    <script src="javascripts/respond.js"></script>
    <!--[if lt IE 9]>
      <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
    <!--[if lt IE 8]>
    <link rel="stylesheet" href="stylesheets/ie.css">
    <![endif]-->
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

  </head>
  <body>
      <div id="header">
        <nav>
          <li class="fork"><a href="https://github.com/mwoodson1/15418FinalProject">View On GitHub</a></li>
          <li class="downloads"><a href="https://github.com/mwoodson1/15418FinalProject/zipball/master">ZIP</a></li>
          <li class="downloads"><a href="https://github.com/mwoodson1/15418FinalProject/tarball/master">TAR</a></li>
          <li class="title">DOWNLOADS</li>
        </nav>
      </div><!-- end header -->

    <div class="wrapper">

      <section>
        <div id="title">
          <h1>NeuroPhi</h1>
          <p></p>
          <hr>
          <span class="credits left">Project maintained by <a href="https://github.com/mwoodson1">mwoodson1</a>,<a href="https://github.com/dmrager">dmrager</a></span>
          <span class="credits right">Hosted on GitHub Pages &mdash; Theme by <a href="https://twitter.com/michigangraham">mattgraham</a></span>
        </div>

        <h3>
<h1>Checkpoint</h1>
<h3>Work Completed So Far</h3>
<p>
After researching how the Caffe framework trains ANNs we realized they use a subclass of ANNs called a Convolutional Neural Network (CNNs). The details of such a network can be found here. CNNs make use of convolutional network layers of spatially contiguous receptive fields over which a single filter is applied. This allows trained CNNs to achieve properties such as translation invariant image identification while tuning relatively few parameters.  After some testing and further research we found the convolutional layer to be the bottle neck of the system. Thus, to improve the speed of the system, we have decided to change our project slightly to optimize this layer. This involves improving the computation of a convolution between an image and given filter.
</p>

<p>
Given the increased complexity of writing a CNN from scratch, we have decided to use base code provided by the ConvNet library, which supports serial CNN training on a CPU and parallel CNN training on a GPU. We will extend this framework to support parallel CPU co-processor computation on the Xeon Phi. We have also been in touch with Cary and professor Kayvon about programming for the Xeon Phi's as that will be our target device. We hope to get a few toy examples working on the Xeon Phi to acquaint ourselves with working with them soon.
</p>

<p>
Both Markus and Danielle were involved in the research of CNN training theory and existing parallel and serial CNN training frameworks. We have since divided the work as follows:
</p>

<p>
Markus: Began benchmarking and writing code to improve the performance of a 2D convolution. We will use Fast Fourier Transform(FFT's) to take the complexity of convolution from O(n^2) to O(nlogn). Along with the complexity decrease the FFT also lends itself quite nicely to parallelism which we think can be exploited both at a high level (threads) and low level (SIMD execution). Currently we have a working naive FFT written in Matlab which we hope to port to C++ soon. 
</p>

<p>
Danielle: Began researching datasets and getting the starter code and examples from the ConvNet code base working on latedays. Given that the codebase runs given a Matlab API and compiled MEX files there were a few issues, but we hope to have them resolved within a day. We will be testing on the <a href="http://www.cs.utoronto.ca/~kriz/cifar.html">CIFAR10/100</a> and <a href="http://ufldl.stanford.edu/housenumbers/">SVHN</a> datasets.
</p>

<h3>New Proposed Schedule</h3>
<table border="1" style="width:100%">
  <tr>
    <td>4/20-4/23</td>
    <td>Get ConvNet example code working and perform initial benchmarks(Danielle). Finish porting Matlab FFT code to C++ and work on parallelization(Markus). Both work on making toy code for Xeon Phi.</td>    
  </tr>
  <tr>
    <td>4/23-4/27</td>
    <td>Benchmark FFT version of neural network code for multiple filter sizes(Danielle). Work on multi-threading the FFT code(Markus).</td> 
  </tr>
  <tr>
    <td>4/27-4/30</td>
    <td>Finish multi-threading and introduce vector intrinsics. Benchmark other parallel FFT implementations to compare to ours.</td>    
  </tr>
    <tr>
    <td>4/30-5/5</td>
    <td>Misc other optimizations we can make to the code and finish up anything remaining. If time permits we would like to train a CNN for larger images such as the ImageNet dataset.</td>    
  </tr>
    <tr>
    <td>5/5-5/11</td>
    <td>Work on presentation and perform final analysis.</td>    
  </tr>
</table>

<h3>Checkpoint Progress</h3>
<p>
Since our project has shifted a bit we are off schedule but believe we should still be able to meet our newly proposed schedule and look forward to the results we get!
</p>

<h3>Competition Demo</h3>
Given that training and running a CNN can take time, even for fast implementations, on demo day we hope to show graphs comparing the performance and training speed of commonly used CNN frameworks such as cudaConvNet and Caffe, to our FFT based CNN. Ideally we hope to show that compute power of CPU's is sufficient for large network training.

<p>
</p>
</body>
</html>